{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rmpybwysXGV"
   },
   "source": [
    "##### Adapted from the official TensorFlow documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 09:57:56.379984: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-05 09:57:56.486055: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "# The Sequential model\n",
    "\n",
    "A `Sequential` model is appropriate for **a plain stack of layers**\n",
    "where each layer has **exactly one input tensor and one output tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Sequential model\n",
    "\n",
    "You can create a Sequential model by passing a list of layers to the Sequential\n",
    "constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.474637   0.47920436 0.47907084]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 09:57:58.074878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "# Call model on a test input\n",
    "x = tf.ones((1,3))\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name=\"mySequentialModel\") # optional name\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the input shape in advance\n",
    "\n",
    "Generally, all layers in Keras need to know the shape of their inputs\n",
    "in order to be able to create their weights. So when you create a layer like\n",
    "this, initially, it has no weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = layers.Dense(3)\n",
    "layer.weights  # Empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates its weights the first time it is called on an input, since the shape\n",
    "of the weights depends on the shape of the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
       " array([[ 0.26556134,  0.00301898,  0.87071157],\n",
       "        [-0.33769238, -0.43853146,  0.5482054 ],\n",
       "        [-0.03606153, -0.50319314, -0.7311568 ],\n",
       "        [-0.7010398 ,  0.35403788,  0.20952606]], dtype=float32)>,\n",
       " <tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call layer on a test input\n",
    "x = tf.ones((1, 4))\n",
    "y = layer(x)\n",
    "layer.weights  # Now it has weights, of shape (4, 3) and (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, this also applies to Sequential models. When you instantiate a\n",
    "Sequential model without an input shape, it isn't \"built\": it has no weights\n",
    "(and calling\n",
    "`model.weights` results in an error stating just this). The weights are created\n",
    "when the model first sees some input data.\n",
    "\n",
    "Once a model is \"built\", you can call its `summary()` method to display its\n",
    "contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it can be very useful when building a Sequential model incrementally\n",
    "to be able to display the summary of the model so far, including the current\n",
    "output shape. \n",
    "\n",
    "In this case, you can either:\n",
    " - start your model by passing an `Input`\n",
    "object to your model, so that it knows its input shape from the start\n",
    " - pass the `input_shape` argument to the layer constructor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(4,)))\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "\n",
    "# ALTERNATIVE:\n",
    "#model = keras.Sequential()\n",
    "#model.add(layers.Dense(2, activation=\"relu\", input_shape=(4,)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: as a debugging strategy when defining complex models, it is useful to call `model.summary()` frequently, between `.add()` calls, to verify that you have not made any mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras *functional API* is a way to create models that are more flexible\n",
    "than the `keras.Sequential` API. The functional API can handle models\n",
    "with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually\n",
    "a directed acyclic graph (DAG) of layers.\n",
    "So the functional API is a way to build *graphs of layers*.\n",
    "\n",
    "To build this model using the functional API, start by creating an input node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"myModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Just for demonstration purposes.\n",
    "inputs = keras.Input(shape=(784,))\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"myModel\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple example, which could be implemented using the Sequential API as well. The Functional API enable things like layer sharing and reuse across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model` class provides built-in APIs for training and validation, which will be enough for your needs most of the time. For advanced use cases, custom training loops can be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passing data to the built-in training loops of a model, you should either use\n",
    "**NumPy arrays** (if your data is small and fits in memory) or **`tf.data.Dataset`\n",
    "objects**. In the next few paragraphs, we'll use the MNIST dataset as NumPy arrays, in\n",
    "order to demonstrate how to use optimizers, losses, and metrics.\n",
    "\n",
    "*The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.*\n",
    "\n",
    "Let's consider the following model (here, we build in with the Functional API, but it\n",
    "could be a Sequential model or a subclassed model as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " digits (InputLayer)         [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 0., 4., ..., 8., 4., 8.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "# Note: reshape is used to convert the 28x28 image to a flat 784-element vector\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the training configuration (optimizer, loss, metrics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3514 - sparse_categorical_accuracy: 0.8998 - val_loss: 0.1979 - val_sparse_categorical_accuracy: 0.9432\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 1s 886us/step - loss: 0.1635 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.1442 - val_sparse_categorical_accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=2,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "79/79 [==============================] - 0s 641us/step - loss: 0.1398 - sparse_categorical_accuracy: 0.9577\n",
      "test loss, test acc: [0.13983562588691711, 0.9577000141143799]\n",
      "Generate predictions for 3 samples\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)#les will be lost.\n",
    "\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically setting apart a validation holdout set\n",
    "\n",
    "In the example, we used the `validation_data` argument to pass\n",
    "a tuple of NumPy arrays `(x_val, y_val)` to the model for evaluating a validation loss\n",
    "and validation metrics at the end of each epoch.\n",
    "\n",
    "Here's another option: the argument `validation_split` allows you to automatically\n",
    "reserve part of your training data for validation. The argument value represents the\n",
    "fraction of the data to be reserved for validation, so it should be set to a number\n",
    "higher than 0 and lower than 1. For instance, `validation_split=0.2` means \"use 20% of\n",
    "the data for validation\", and `validation_split=0.6` means \"use 60% of the data for\n",
    "validation\".\n",
    "\n",
    "The way the validation is computed is by taking the last x% samples of the arrays\n",
    "received by the `fit()` call, before any shuffling.\n",
    "\n",
    "Note that you can only use `validation_split` when training with NumPy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on `tf.data` Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays are the best option to pass input data in most the situations (especially if the dataset is small enough to fit into memory).\n",
    "You may want to use a `tf.data.Dataset` if you have large datasets and you need to do distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 1s 772us/step - loss: 0.1163 - sparse_categorical_accuracy: 0.9658\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 1s 746us/step - loss: 0.0924 - sparse_categorical_accuracy: 0.9723\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 754us/step - loss: 0.0759 - sparse_categorical_accuracy: 0.9773\n",
      "Evaluate\n",
      "157/157 [==============================] - 0s 525us/step - loss: 0.1105 - sparse_categorical_accuracy: 0.9679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.11046934872865677,\n",
       " 'sparse_categorical_accuracy': 0.9678999781608582}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Now we get a test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# You can also evaluate or predict on a dataset.\n",
    "print(\"Evaluate\")\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a validation set during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 843us/step - loss: 0.0650 - sparse_categorical_accuracy: 0.9803 - val_loss: 0.1017 - val_sparse_categorical_accuracy: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63f045ad40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks in Keras are objects that are called at different points during training (at\n",
    "the start of an epoch, at the end of a batch, at the end of an epoch, etc.). They\n",
    "can be used to implement certain behaviors, such as:\n",
    "\n",
    "- Doing validation at different points during training (beyond the built-in per-epoch\n",
    "validation)\n",
    "- Checkpointing the model at regular intervals or when it exceeds a certain accuracy\n",
    "threshold\n",
    "- Changing the learning rate of the model when training seems to be plateauing\n",
    "- Doing fine-tuning of the top layers when training seems to be plateauing\n",
    "- Sending email or instant message notifications when training ends or where a certain\n",
    "performance threshold is exceeded\n",
    "\n",
    "### Built-in callbacks\n",
    "There are many built-in callbacks already available in Keras, such as:\n",
    "\n",
    "- `ModelCheckpoint`: Periodically save the model.\n",
    "- `EarlyStopping`: Stop training when training is no longer improving the validation\n",
    "metrics.\n",
    "- `TensorBoard`: periodically write model logs that can be visualized in\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard).\n",
    "- `CSVLogger`: streams loss and metrics data to a CSV file.\n",
    "\n",
    "### Writing your own callback\n",
    "\n",
    "You can create a custom callback by extending the base class\n",
    "`keras.callbacks.Callback`. A callback has access to its associated model through the\n",
    "class property `self.model`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basics.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
